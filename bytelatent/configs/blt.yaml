# Template config, need to change dump_dir, data.root_dir and tokenizer.path
# Evals can be activated by uncommenting its config
# python -m launchers.stool config=apps/main/configs/debug.yaml nodes=8 account=fair_amaia_cw_codegen qos=lowest
dump_dir: ./ckpt/blt-entropy-400m
name: "blt-400m"
steps: 100_001
probe_freq: null
seed: 777
debug_dynamo: true
optim:
  lr: 4e-04
  warmup: 500
  lr_min_ratio: 0.1
  clip: 10.0
distributed:
  fsdp_type: no_shard
  model_dtype: bf16
  matmul_allow_tf32: true
  selective_activation_checkpointing: true
  tp_size: 1
  compile: true
model:
  n_heads: 12
  n_heads_global: 10
  n_heads_local_encoder: 12
  n_heads_local_decoder: 12
  n_layers_global: 24
  n_layers_local_encoder: 1
  n_layers_local_decoder: 7
  dim: 768
  dim_global: 1280
  vocab_size: 260
  dim_token: 768
  patch_size: 6
  patching_mode: "entropy"
  monotonicity: true
  tie_local_encoder_decoder_logits: false
  patch_in_forward: false
  max_encoder_seq_length: 8192
  pad_to_max_length: true
  patching_threshold: 3.1439168453216553
  encoder_hash_byte_group_size: [4]
  encoder_hash_byte_group_vocab: 50002
  encoder_hash_byte_group_nb_functions: 3
  encoder_enable_byte_ngrams: false
  # ngram_vocab_sizes: 50000
  # encoder_ngram_to_size: 3
  cross_attn_encoder: true # assuming cross_attention is true
  cross_attn_decoder: true # assuming cross_attention is true
  cross_attn_window_encoder: 512
  cross_attn_window_decoder: 512
  dim_local_encoder: 768
  dim_local_decoder: 768
  cross_attn_k: 2
  cross_attn_nheads: 6
  cross_attn_all_layers_decoder: true
  cross_attn_all_layers_encoder: true
  cross_attn_use_flex_attention: true
  cross_attn_init_by_pooling: true
  log_patch_lengths: true
  non_linearity: "swiglu"
  use_rope: true
  recompute_fc1_out: true
  recompute_fc3_out: true
  recompute_attn: true
  custom_bwd: false
  layer_ckpt: "none"
  use_local_encoder_transformer: true
  init_use_gaussian: true
  init_use_depth: "current"
  attn_impl: "xformers"
  attn_bias_type: "block_causal"
  alpha_depth: "disabled"
  max_length: 256
  local_attention_window_len: 512
  max_seqlen: 8192
  downsampling_by_pooling: "max"
  use_fsdp: false
data:
  root_dir: ./data
  sources:
    train: 1.0
  batch_size: 1
  patcher_args:
    threshold: 3.1439168453216553
  prefetch_size: 32
  seq_len: 2048
  load_async: true
  preprocess_dir: ./data/preprocessed
  max_encoder_seq_length: 8192
  async_persist_type: approximate
  tokenizer_args:
    name: blt
    init_kwargs:
      bpe_tokenizer_path: ./data/tokenizers
profiling:
  run: false
checkpoint:
  dump:
    every: 1000
    keep: 3
  eval:
    every: 1
    keep: -1
logging:
  freq: 1
  wandb:
    name: blt
    project: bytelatent
    entity: danielpwarren


async_eval_gpus: null
eval_on_gpus: null
eval:
  dump_dir: ./eval_results
  ckpt_dir: ./ckpt/blt-entropy-400m/checkpoints/0000100000
  run_ppl: true
  run_tasks: true

  generator:
    compile_prefilling: false
    reduce_generation_overhead: false
    device: "cuda"

  validation:
    max_n_docs: null  # Use entire validation set
    batch_size: 1
    root_dir: ./data/val
    sources:
      - 00.val.jsonl.shard_00.arrow

  harness:
    tasks: ["hellaswag", "mmlu"]
    num_fewshot: 0
